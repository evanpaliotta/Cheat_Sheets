{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4 Steps: Preprocess, Train, Validate, Predict\n",
    "* .ravel() function creates a continuous, 1D, flattened array\n",
    "* Day 1, Activity 6 has many details we haven't explored much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression is a statistical method for predicting binary outcomes from data.\n",
    "* Examples of this are \"yes\" vs \"no\" or \"high credit risk\" vs \"low credit risk\".\n",
    "* These are categories that translate to probability of being a 0 or a 1\n",
    "* We can calculate logistic regression by adding an activation function as the final step to our linear model.\n",
    "* This converts the linear regression output to a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Generate/Import Data\n",
    "2. Split Data (X_train, X_test, y_train, y_test)\n",
    "3. Create Model\n",
    "4. Fit/Train Model\n",
    "5. Validate Model\n",
    "6. Make Predicitions\n",
    "7. Confusion Matrix\n",
    "8. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs #to generate random, clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - create model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - train the data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - score/validate the model\n",
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - make predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}) #.reset_index(drop=True) #Might need to reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 - confustion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 - classification report\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Group 1\", \"Group 2\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data\n",
    "* In other words, SVM tries to optimally slice the data\n",
    "* The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "* Same as LR with a different model in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for a linear SVM model\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a graphical representation of possible solutions to a decision based on certain conditions--they encode a series of true/false questions\n",
    "* tree based algorithms map non-linear relationships in data\n",
    "* hierarchical if else statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Generate/Import Data\n",
    "2. Preprocess Data (drop, reshape, scale, transform etc.)\n",
    "3. Fit the Model\n",
    "4. Make Predicitions\n",
    "5. Model Evaluation\n",
    "6. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Needed for decision tree visualization\n",
    "import pydotplus\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - various preprocess code\n",
    "\n",
    "# Define features set\n",
    "X = df.copy()\n",
    "X.drop(\"Column\", axis=1, inplace=True)\n",
    "\n",
    "# Define target vector\n",
    "y = df[\"Column\"].values.reshape(-1, 1)\n",
    "\n",
    "# Splitting into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the Standard Scaler with the training data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the training data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - fit\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model\n",
    "model = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - predict\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - evaluate\n",
    "\n",
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - visualize\n",
    "\n",
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(\n",
    "    model, out_file=None, feature_names=X.columns, class_names=[\"0\", \"1\"], filled=True\n",
    ")\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* each tree is simpler because it is built from a subset of data\n",
    "* when weak classifiers are combined, it can create a robust model against overfitting\n",
    "* random forest runs efficiently on large databases\n",
    "* feature importance describes which trees were most successful at predicition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "* Same as Decision Tree with different model and no visualization\n",
    "* Feature Importance instead of visualizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Needed for decision tree visualization\n",
    "import pydotplus\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for random forrest model\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=500, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests in sklearn will automatically calculate feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# We can sort the features by their importance\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
